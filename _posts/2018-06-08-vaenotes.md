---
title: Notes on Implementing a Variational Autoencoder
desc: Dealing with the pesky Kullbeck-Liebler Divergence calculations
comments: true
---

*Note: This blog post accompanies code here that demonstrates a VAE through a pure numpy implementation.*

Generative models, in short, aims examine and learn characteristics existing data, and generate new samples of data that are similar, but not exactly the same, as the dataset they examined. They have gained significant academic interest, and resulted in useful, fascinating, a perhaps disturbing applications of machine learning. With the rise of deep learning techniques like stochastic gradient descent and backpropagation, Variational Autoencoders have risen in popularity as an unsupervised latent variable model able to generate complicated probability distributions that define the underlying data characteristics. This blog does not aim to explain VAEs and how they work, but will rather expound on a couple of technical details in understanding it’s implementation.  There are plenty of great blog posts explaining VAEs. Or you can read the original paper. Or papers explaining the paper.

### Calculating KL Loss

The KL component of the loss function usually relies on assuming the true distribution of Q(z&#124;X) as a Spherical Gaussian prior. Though there is ongoing research on other types of priors, Gaussians are often used because they are computationally straightforward. The KL-Divergence between two multivariate Gaussians can be computed in closed form.

A lot of VAE examples usually write the code plainly as:

    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

But I haven’t seen explanations for where this comes from. This can be derived like so:



### Backpropagating KL Loss

Backpropagating the KL loss was a bit of a pain, because matrix calculus is confusing, especially when there is a summation involved in the equation. However, I found a rather elegant mathematical solution which relies on properties of Hadamard (elementwise) and Frobenious (trace) products for matrices.

---
